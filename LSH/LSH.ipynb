{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_gram_list(s, shingle_len=3):\n",
    "    #s = s.replace('-', ' ')\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^\\w\\s]','',s)\n",
    "    tokens = s.split()\n",
    "    return [tuple(tokens[i:i+shingle_len]) for i in range(len(tokens) - shingle_len+1)]\n",
    "\n",
    "def get_all_shingles(folder_dir='./athletics'):\n",
    "    shingles = set()\n",
    "    sorted_listdir = sorted(os.listdir(folder_dir))\n",
    "    \n",
    "    for fl in sorted_listdir:\n",
    "        fl_path = os.path.join(folder_dir, fl)\n",
    "        if fl_path[-4:] == '.txt':\n",
    "            with open(fl_path) as f:\n",
    "                for line in f:\n",
    "                    for kgram in k_gram_list(line):\n",
    "                        shingles.add(kgram) \n",
    "\n",
    "    #print(len(shingles)) # 25612\n",
    "    return  shingles\n",
    "\n",
    "# get_all_shingles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input(f):\n",
    "    file_name = f[0].split('/')[-1].replace('.txt', '')\n",
    "    all_kgrams = []\n",
    "    for line in f[1].split('\\n'):\n",
    "        if line != '':\n",
    "            line = re.sub(r'\\'', '', line)\n",
    "            all_kgrams.extend(k_gram_list(line))\n",
    "        \n",
    "    return file_name, all_kgrams\n",
    "    \n",
    "\n",
    "def has_shingle(x):\n",
    "    s_id, shingle = x[0][0], x[0][1]\n",
    "    d_id, d_list = x[1][0], x[1][1]\n",
    "    \n",
    "    if shingle in d_list:\n",
    "        return d_id, (s_id, 1)\n",
    "    else:\n",
    "        return d_id, (s_id, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"pyspark-lsh\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "folder_dir = './athletics'\n",
    "\n",
    "\"\"\" Shingling \"\"\"\n",
    "# (doc_id, [all k_grams tuples])\n",
    "docs_rdd = sc.wholeTextFiles(folder_dir).map(parse_input).sortByKey()\n",
    "# docs_rdd.take(5)\n",
    "\n",
    "shingles = get_all_shingles()\n",
    "\n",
    "# (shing_id, shingle tuple)\n",
    "shingles_rdd = sc.parallelize(shingles).zipWithIndex().map(lambda x:(x[1], x[0]))\n",
    "\n",
    "joined = shingles_rdd.cartesian(docs_rdd) \\\n",
    "        .map(has_shingle)\n",
    "\n",
    "characteristic_row = joined.groupByKey().sortByKey().cache()\n",
    "\n",
    "# (doc_id, [shing_ids that (shing_id, doc_id)==1])\n",
    "filtered = joined.filter(lambda x:x[1][1]==1) \\\n",
    "        .mapValues(lambda x:x[0]).groupByKey()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generate 100 hash functions to simulate the permutation step\n",
    "random.seed(42)\n",
    "a = random.sample(range(1,10000), 100)\n",
    "b = random.sample(range(1,10000), 100)\n",
    "\n",
    "N = len(shingles) # 25612\n",
    "p = 25999 # a prime number > N\n",
    "\n",
    "num_bands = 50\n",
    "num_buckets = 10000 # number of buckets to hash to\n",
    "\n",
    "\n",
    "def min_hash(a, b, shing_ids):\n",
    "    hashed_rows = [((a*int(r) + b) % p) % N for r in shing_ids]\n",
    "    return min(hashed_rows)\n",
    "\n",
    "def get_signature(x):\n",
    "    doc_id, shing_ids = x\n",
    "    return [((doc_id, i%num_bands), min_hash(a[i], b[i], shing_ids)) for i in range(100)]\n",
    "\n",
    "# hash table for 2 rows per band\n",
    "def band_hasher(x):\n",
    "    return (((631*x[0]+641*x[1])<<1) % 10099) % num_buckets \n",
    "\n",
    "def hash_to_bucket(x):\n",
    "    doc_id, band_id, sig_list = x[0][0], x[0][1], x[1]\n",
    "    return ((band_id, band_hasher(sig_list)), [doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Min-hashing \"\"\"\n",
    "signatures = filtered.flatMap(get_signature)\n",
    "\n",
    "\"\"\" Locality-sensitive hashing \"\"\"\n",
    "# ((d_id, band_id), [sig vals])\n",
    "bands = signatures.groupByKey().mapValues(list)\n",
    "\n",
    "bands = bands.map(hash_to_bucket) \\\n",
    "            .reduceByKey(lambda a, b: a + b)\\\n",
    "            .filter(lambda x: len(x[1]) > 1)\n",
    "\n",
    "#bands.take(20) # ((band_id, bucket_id), [doc_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_mapper(x):\n",
    "    similar_set, index = x[0], x[1]\n",
    "    return map(lambda ele: (ele, index), similar_set)\n",
    "\n",
    "# use frozenset since our key is a list(mutable)\n",
    "candidates = bands.map(lambda x: frozenset(sorted(x[1]))).distinct() \\\n",
    "                .zipWithIndex().flatMap(candidates_mapper)\n",
    "# (doc_id, band_id)\n",
    "# ('049', 0),('048', 0),('079', 1),('081', 1),('095', 2),('076', 2),..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add shingles information by doc_id\n",
    "doc_vectors = candidates.join(characteristic_row) \\\n",
    "                        .map(lambda x: (x[1][0], (x[0], x[1][1])))\n",
    "# (0, ('049', <pyspark.resultiterable.ResultIterable at 0x10a292c40>))\n",
    "\n",
    "# same as using groupByKey(), but more efficient\n",
    "# output: (bucket_id, [(doc_id, characteristic), ...])\n",
    "bucket_vectors = doc_vectors.mapValues(lambda x:[x]).reduceByKey(lambda a,b:a+b)\n",
    "\n",
    "# bucket_vectors.sortBy(lambda x: x[0]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def calculate_jaccard_sim(x):\n",
    "    bucket_id, cdt_list = x\n",
    "\n",
    "    idxs = [i for i in range(len(cdt_list))]\n",
    "    lst = []\n",
    "    for e1, e2 in itertools.combinations(idxs,2):\n",
    "        sum_list = [a[1]+b[1] for a, b in zip(cdt_list[e1][1], cdt_list[e2][1])]\n",
    "        sim = sum_list.count(2) / ( sum_list.count(2)+sum_list.count(1))\n",
    "        lst.append( ((cdt_list[e1][0], cdt_list[e2][0]), sim)) \n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "bucket_vect_with_sim = bucket_vectors.flatMap(calculate_jaccard_sim)\n",
    "\n",
    "# get distinct keys, and sort by similarity in decreasing order\n",
    "ans = bucket_vect_with_sim.reduceByKey(lambda x,y : x).sortBy(lambda x : -x[1]).take(10)\n",
    "#ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('052', '084'): 1.0\n",
      "('020', '012'): 1.0\n",
      "('049', '047'): 0.7576576576576577\n",
      "('030', '035'): 0.7097097097097097\n",
      "('049', '088'): 0.5165876777251185\n",
      "('049', '048'): 0.4839476813317479\n",
      "('023', '038'): 0.4804177545691906\n",
      "('040', '014'): 0.40238704177323104\n",
      "('088', '047'): 0.3917340521114106\n",
      "('047', '048'): 0.36666666666666664\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'w') as of:\n",
    "    for l in ans:\n",
    "        print(f'{l[0]}: {l[1]}')\n",
    "        of.write(f'{l[0]}: {l[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
